{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the various dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-29 17:03:24.989365: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-29 17:03:24.989405: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from scandir import scandir\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Model as KerasModel\n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from lib_1.PixelShuffler import PixelShuffler\n",
    "from lib_1.training_data import minibatchAB, stack_images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the caracteristics of our network:\n",
    "   * sav_Model: the location of the model (it needs to be the absolute path)\n",
    "   * pretrained_weight: transfer learning can be used to speed up the learning process\n",
    "   * sav_Model: the location of the model (it needs to be the absolute path)\n",
    "   * image_actor_A_directory: the location of the training data for the actor A\n",
    "   * image_actor_B_directory: the location of the training data for the actor B\n",
    "   * batch_size: the batch size (1<< batch_size <<your training dataset size)\n",
    "       * a small batch_size enables poor hardware to run the training but the gradiant might be a bit noisy. I advice you to pick a batch size of 32 or 64\n",
    "   * save_interval: it define at which interval of time Keras should save your model\n",
    "   * IMAGE_SHAPE: the shape of the input image\n",
    "       * it must be consistant with the one you use during the prediction\n",
    "   * ENCODER_DIM: the dimension of the encoding in the autoencoder\n",
    "       * it must be consistant with the one you use during the prediction\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sav_Model=\"/home/somya51p/Desktop/swap-face/deepfakes/weight_repo\"\n",
    "pretrained_weight=\"/home/somya51p/Desktop/swap-face/deepfakes/weight\"\n",
    "image_actor_A_directory=\"/home/somya51p/Desktop/swap-face/daniel_craig/\"\n",
    "image_actor_B_directory=\"/home/somya51p/Desktop/swap-face/pierce_broceman/\"\n",
    "batch_size=1\n",
    "save_interval=100\n",
    "ENCODER_DIM = 1024\n",
    "\n",
    "\n",
    "#DON'T MODIFY\n",
    "image_extensions = [\".jpg\", \".jpeg\", \".png\"]\n",
    "encoderH5 = '/encoder.h5'\n",
    "decoder_AH5 = '/decoder_A.h5'\n",
    "decoder_BH5 = '/decoder_B.h5'\n",
    "IMAGE_SHAPE = (64, 64, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the model and its associated trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model_dir = sav_Model\n",
    "        self.pretrained_weight=pretrained_weight\n",
    "        self.encoder = self.Encoder()\n",
    "        self.decoder_A = self.Decoder()\n",
    "        self.decoder_B = self.Decoder()\n",
    "\n",
    "        self.initModel()\n",
    "\n",
    "    \n",
    "    def initModel(self):\n",
    "        optimizer = Adam(lr=5e-5, beta_1=0.5, beta_2=0.999)\n",
    "        x = Input(shape=IMAGE_SHAPE)\n",
    "\n",
    "        self.autoencoder_A = KerasModel(x, self.decoder_A(self.encoder(x)))\n",
    "        self.autoencoder_B = KerasModel(x, self.decoder_B(self.encoder(x)))\n",
    "\n",
    "        self.autoencoder_A.compile(optimizer=optimizer, loss='mean_absolute_error')\n",
    "        self.autoencoder_B.compile(optimizer=optimizer, loss='mean_absolute_error')\n",
    "\n",
    "    def converter(self, swap):\n",
    "        autoencoder = self.autoencoder_B if not swap else self.autoencoder_A \n",
    "        return lambda img: autoencoder.predict(img)\n",
    "\n",
    "    def conv(self, filters):\n",
    "        def block(x):\n",
    "            x = Conv2D(filters, kernel_size=5, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(0.1)(x)\n",
    "            return x\n",
    "        return block\n",
    "\n",
    "    def upscale(self, filters):\n",
    "        def block(x):\n",
    "            x = Conv2D(filters * 4, kernel_size=3, padding='same')(x)\n",
    "            x = LeakyReLU(0.1)(x)\n",
    "            x = PixelShuffler()(x)\n",
    "            return x\n",
    "        return block\n",
    "\n",
    "    def Encoder(self):\n",
    "        input_ = Input(shape=IMAGE_SHAPE)\n",
    "        x = input_\n",
    "        x = self.conv(128)(x)\n",
    "        x = self.conv(256)(x)\n",
    "        x = self.conv(512)(x)\n",
    "        x = self.conv(1024)(x)\n",
    "        x = Dense(ENCODER_DIM)(Flatten()(x))\n",
    "        x = Dense(4 * 4 * 1024)(x)\n",
    "        x = Reshape((4, 4, 1024))(x)\n",
    "        x = self.upscale(512)(x)\n",
    "        return KerasModel(input_, x)\n",
    "\n",
    "    def Decoder(self):\n",
    "        input_ = Input(shape=(8, 8, 512))\n",
    "        x = input_\n",
    "        x = self.upscale(256)(x)\n",
    "        x = self.upscale(128)(x)\n",
    "        x = self.upscale(64)(x)\n",
    "        x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)\n",
    "        return KerasModel(input_, x)\n",
    "        \n",
    "    def load(self, swapped):\n",
    "        (face_A,face_B) = (decoder_AH5, decoder_BH5) if not swapped else (decoder_BH5, decoder_AH5)\n",
    "\n",
    "        try:\n",
    "            self.encoder.load_weights(self.pretrained_weight + encoderH5)\n",
    "            self.decoder_A.load_weights(self.pretrained_weight + face_A)\n",
    "            self.decoder_B.load_weights(self.pretrained_weight + face_B)\n",
    "            print('loaded model weights')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print('Failed loading existing training data.')\n",
    "            print(e)\n",
    "            return False\n",
    "\n",
    "    def save_weights(self):\n",
    "        self.encoder.save_weights(self.model_dir + encoderH5)\n",
    "        self.decoder_A.save_weights(self.model_dir + decoder_AH5)\n",
    "        self.decoder_B.save_weights(self.model_dir + decoder_BH5)\n",
    "        print('saved model weights')\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, fn_A, fn_B, batch_size=64):\n",
    "        self.batch_size = batch_size\n",
    "        self.model = model\n",
    "        self.images_A = minibatchAB(fn_A, self.batch_size)\n",
    "        self.images_B = minibatchAB(fn_B, self.batch_size)\n",
    "\n",
    "    def train_one_step(self, iter):\n",
    "        epoch, warped_A, target_A = next(self.images_A)\n",
    "        epoch, warped_B, target_B = next(self.images_B)\n",
    "\n",
    "        loss_A = self.model.autoencoder_A.train_on_batch(warped_A, target_A)\n",
    "        loss_B = self.model.autoencoder_B.train_on_batch(warped_B, target_B)\n",
    "        print(\"[{0}] [#{1:05d}] loss_A: {2:.5f}, loss_B: {3:.5f}\".format(time.strftime(\"%H:%M:%S\"), iter, loss_A, loss_B),\n",
    "            end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(directory):\n",
    "    return [x.path for x in scandir(directory) if\n",
    "     any(map(lambda ext: x.name.lower().endswith(ext), image_extensions))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We launch the training. We will try to minimize th loss of the auto encoder A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data, this may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-29 17:04:08.118658: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-29 17:04:08.215336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-09-29 17:04:08.618151: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-09-29 17:04:08.618207: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (somya51p-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2021-09-29 17:04:08.620157: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-29 17:04:08.620664: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-29 17:04:13.642831: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-29 17:04:13.663147: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1800000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model weights loss_A: 0.17936, loss_B: 0.16923\n",
      "saved model weights loss_A: 0.08291, loss_B: 0.07512\n",
      "saved model weights loss_A: 0.07684, loss_B: 0.04909\n",
      "saved model weights loss_A: 0.04895, loss_B: 0.06144\n",
      "saved model weights loss_A: 0.04379, loss_B: 0.06248\n",
      "[17:14:45] [#00499] loss_A: 0.06155, loss_B: 0.05755\r"
     ]
    }
   ],
   "source": [
    "print('Loading data, this may take a while...')\n",
    "# this is so that you can enter case insensitive values for trainer\n",
    "\n",
    "model = Model()\n",
    "model.load(swapped=False)\n",
    "\n",
    "images_A = get_image_paths(image_actor_A_directory)\n",
    "images_B = get_image_paths(image_actor_B_directory)\n",
    "trainer = Trainer(model,images_A,images_B,batch_size=batch_size)\n",
    "\n",
    "for epoch in range(0, 500):\n",
    "\n",
    "    save_iteration = epoch % save_interval == 0\n",
    "\n",
    "    trainer.train_one_step(epoch)\n",
    "\n",
    "    if save_iteration:\n",
    "        model.save_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
